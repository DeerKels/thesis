{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gEFJi2q8NpZ",
        "outputId": "aa0c476b-95dd-4b12-deb5-be2834f2de63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from pandas_datareader import data as pdr\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Logit Regression on Training set\n",
        "\n"
      ],
      "metadata": {
        "id": "kl2TR2Sz8V4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Training = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Train.csv', converters={'Global Company Key':str})"
      ],
      "metadata": {
        "id": "sKaUQB_K8Z6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_y = Training['Selected']\n",
        "Train_X = Training.drop(columns=['Selected'])\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(Train_X, Train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Ybzl5RMP8hI9",
        "outputId": "7a366db1-999b-497c-c3d6-5472edb5b694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Logit Regression using Test set"
      ],
      "metadata": {
        "id": "MU8UiXoR84EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Test = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Test_filled_and_cleaned.csv', index_col=0, converters={'Global Company Key':str})"
      ],
      "metadata": {
        "id": "QwaxK5Gn85Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict_proba(Test[Train_X.columns])\n",
        "Test['Selected']= [row[1] for row in predictions]"
      ],
      "metadata": {
        "id": "CoAPYJMd9H3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sort them into quintiles by 'Data Date' and then by 'Selected'"
      ],
      "metadata": {
        "id": "osdp5my3UAmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Test = Test.sort_values(by=['Data Date', 'Selected'], ascending = [True, False])"
      ],
      "metadata": {
        "id": "UuZB_vfiUAJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtain_Ticker(group):\n",
        "    return group.head(len(group) // 5)['Ticker Symbol'].tolist()"
      ],
      "metadata": {
        "id": "YnK1BpQwUoFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group DataFrame by 'Data Date' and apply the custom function\n",
        "Tickers = Test.groupby('Data Date').apply(obtain_Ticker)\n",
        "Dates = Test['Data Date'].unique()"
      ],
      "metadata": {
        "id": "6she0CTVbX3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio1 = pd.DataFrame()\n",
        "\n",
        "for i in range(300):\n",
        "  print(i)\n",
        "#for i in range(36):\n",
        "  start_date = Dates[i]\n",
        "\n",
        "  if (i+1) < len(Dates):\n",
        "    end_date = (datetime.strptime(Dates[i+1], '%Y-%m-%d') + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  else:\n",
        "    end_date = (datetime.strptime(Dates[i], '%Y-%m-%d') + relativedelta(months=1) + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  print(start_date)\n",
        "  print(end_date)\n",
        "\n",
        "  for ticker in Tickers[i]:\n",
        "    try:\n",
        "      ticker_data = yf.download(ticker, start=start_date, end=end_date)['Adj Close']\n",
        "      if not ticker_data.empty:\n",
        "        df[ticker] = ticker_data\n",
        "        df[ticker] = df[ticker].fillna(method='ffill')\n",
        "        df[ticker] = df[ticker].pct_change()\n",
        "        df['Equal_Weighted_Returns'] = df.mean(axis=1)\n",
        "    except Exception as e:\n",
        "      print('exception e')\n",
        "\n",
        "  if not df.empty:\n",
        "    replicating_portfolio1 = pd.concat([replicating_portfolio1, df['Equal_Weighted_Returns']], axis=0)\n",
        "\n",
        "replicating_portfolio1"
      ],
      "metadata": {
        "id": "Zo7CCPRdUfiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio1.to_csv('Logit_rep1.csv')"
      ],
      "metadata": {
        "id": "FnBaHQY4A-EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio2 = pd.DataFrame()\n",
        "\n",
        "for i in range(300, 400):\n",
        "  print(i)\n",
        "#for i in range(36):\n",
        "  start_date = Dates[i]\n",
        "\n",
        "  if (i+1) < len(Dates):\n",
        "    end_date = (datetime.strptime(Dates[i+1], '%Y-%m-%d') + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  else:\n",
        "    end_date = (datetime.strptime(Dates[i], '%Y-%m-%d') + relativedelta(months=1) + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  print(start_date)\n",
        "  print(end_date)\n",
        "\n",
        "  for ticker in Tickers[i]:\n",
        "    try:\n",
        "      ticker_data = yf.download(ticker, start=start_date, end=end_date)['Adj Close']\n",
        "      if not ticker_data.empty:\n",
        "        df[ticker] = ticker_data\n",
        "        df[ticker] = df[ticker].fillna(method='ffill')\n",
        "        df[ticker] = df[ticker].pct_change()\n",
        "        df['Equal_Weighted_Returns'] = df.mean(axis=1)\n",
        "    except Exception as e:\n",
        "      print('exception e')\n",
        "\n",
        "  if not df.empty:\n",
        "    replicating_portfolio2 = pd.concat([replicating_portfolio2, df['Equal_Weighted_Returns']], axis=0)\n",
        "\n",
        "replicating_portfolio2"
      ],
      "metadata": {
        "id": "w7S1mbB3U3MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio2.to_csv('Logit_rep2.csv', index=True)"
      ],
      "metadata": {
        "id": "7O1uIaLvBxJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio3 = pd.DataFrame()\n",
        "\n",
        "for i in range(400, len(Tickers)):\n",
        "  print(i)\n",
        "#for i in range(36):\n",
        "  start_date = Dates[i]\n",
        "\n",
        "  if (i+1) < len(Dates):\n",
        "    end_date = (datetime.strptime(Dates[i+1], '%Y-%m-%d') + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  else:\n",
        "    end_date = (datetime.strptime(Dates[i], '%Y-%m-%d') + relativedelta(months=1) + relativedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  print(start_date)\n",
        "  print(end_date)\n",
        "\n",
        "  for ticker in Tickers[i]:\n",
        "    try:\n",
        "      ticker_data = yf.download(ticker, start=start_date, end=end_date)['Adj Close']\n",
        "      if not ticker_data.empty:\n",
        "        df[ticker] = ticker_data\n",
        "        df[ticker] = df[ticker].fillna(method='ffill')\n",
        "        df[ticker] = df[ticker].pct_change()\n",
        "        df['Equal_Weighted_Returns'] = df.mean(axis=1)\n",
        "    except Exception as e:\n",
        "      print('exception e')\n",
        "\n",
        "  if not df.empty:\n",
        "    replicating_portfolio3 = pd.concat([replicating_portfolio3, df['Equal_Weighted_Returns']], axis=0)\n",
        "\n",
        "df\n",
        "replicating_portfolio3"
      ],
      "metadata": {
        "id": "p_g0eDM8EFfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replicating_portfolio3.to_csv('Logit_rep3.csv', index=True)"
      ],
      "metadata": {
        "id": "itt9qqy9EK6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to concat the three csvs together"
      ],
      "metadata": {
        "id": "qCdjseqEmVKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rep1 = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Logit_rep1.csv')\n",
        "rep2 = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Logit_rep2.csv')\n",
        "rep3 = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Logit_rep3.csv')"
      ],
      "metadata": {
        "id": "lJODBQFYmZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep = pd.concat([rep1, rep2, rep3])\n",
        "rep = rep.rename(columns={'0': 'returns'})\n",
        "rep = rep.dropna()"
      ],
      "metadata": {
        "id": "kWElM8TgqHEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep.to_csv('Logit_replicating.csv')"
      ],
      "metadata": {
        "id": "uAoWdjJ2qliH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below we run a Sparse Logit Regression"
      ],
      "metadata": {
        "id": "cY5MZnp2QvOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/Senior Fall/Senior Thesis/Data/Separately/Train_big.csv', converters={'Global Company Key':str})"
      ],
      "metadata": {
        "id": "U4oI5wu7Rn9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop(X.columns[-1], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "4KAbVGHvTc6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(X, test_size=0.25, random_state=42)\n",
        "\n",
        "y_train = X_train['Selected']\n",
        "X_train = X_train.drop(columns=['Selected'])\n",
        "\n",
        "y_test = X_test['Selected']\n",
        "X_test = X_test.drop(columns=['Selected'])"
      ],
      "metadata": {
        "id": "Qven7xQGRl4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
        "lambda_values = [0.01, 0.1, 1.0, 10.0]\n",
        "results = {\n",
        "    'lambda': [],\n",
        "    'accuracy': [],\n",
        "    'MAE': [],\n",
        "    'RMSE': []\n",
        "}\n",
        "for lambda_val in lambda_values:\n",
        "  print(lambda_val)\n",
        "  # Creating a logistic regression model with L1 penalty (for sparsity) and setting C = 1/lambda\n",
        "  model = LogisticRegression(penalty='l1', C=1/lambda_val, solver='liblinear', random_state=42)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Calculating accuracy\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # Calculating mean absolute error\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "  # Calculating root mean squared error\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "  # Storing results\n",
        "  results['lambda'].append(lambda_val)\n",
        "  results['accuracy'].append(accuracy)\n",
        "  results['MAE'].append(mae)\n",
        "  results['RMSE'].append(rmse)\n",
        "\n",
        "# Printing results\n",
        "print(\"Results:\")\n",
        "print(\"Lambda   Accuracy   MAE       RMSE\")\n",
        "\n",
        "for i in range(len(lambda_values)):\n",
        "    print(f\"{results['lambda'][i]:.2f}     {results['accuracy'][i]:.4f}     {results['MAE'][i]:.4f}     {results['RMSE'][i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIG3gjOySVg1",
        "outputId": "8dd378fa-6814-4cab-9671-6836dc1df290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "10.0\n",
            "Results:\n",
            "Lambda   Accuracy   MAE       RMSE\n",
            "0.01     0.9970     0.0030     0.0552\n",
            "0.10     0.9970     0.0030     0.0543\n",
            "1.00     0.9969     0.0031     0.0555\n",
            "10.00     0.9970     0.0030     0.0543\n"
          ]
        }
      ]
    }
  ]
}